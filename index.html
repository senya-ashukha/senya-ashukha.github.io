<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Senya is Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-167092781-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-167092781-1');
  </script>

  <script type="text/javascript">
   function visibility_on(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'none')
            e.style.display = 'block';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'none')
            e.style.display = 'block';
   }
   function visibility_off(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'block')
            e.style.display = 'none';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'block')
            e.style.display = 'none';
   }
   function toggle_visibility(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
   }
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
</script>


  <title>Senya Ashukha</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Arsenii (Senya) Ashukha</name>
              </p>
              <p>I am a PhD candidate at <a href="https://bayesgroup.ru"> Bayesian methods research group</a> and <a href="https://research.samsung.com/aicenter_moscow">Samsung AI Center Moscow</a> with <a href="https://bayesgroup.ru/people/dmitry-vetrov/">Dmitriy Vetrov</a>, where I work on probabilistic deep learning. 
              </p>
              <p align=center>
                <a href="mailto:ars.ashuha@gmail.com" target="_blank">Email</a> &nbsp/&nbsp
                <a href="https://senya-ashukha.github.io/arsenii-ashukha-cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=IU-kuP8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/senya-ashukha">GitHub</a> &nbsp/&nbsp
                <a href="https://twitter.com/senya_ashuha">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/images/senya-ashukha.jpg"><img style="border-radius:50%;width:100%;max-width:100%" alt="profile photo" src="images/senya-ashukha.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in core deep learning, probabilistic inference, uncertainty estimation, and learning with limited data. 
                My works have been focused on understanding and applications of variational inference in deep neural networks. 
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:10px;width:25%;vertical-align:middle">
                <img src='projects/gps_uai20/gps.png' width="170px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2002.09103">
                <papertitle>Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov*</a>,
              <a href="https://scholar.google.ru/citations?user=5LXTi40AAAAJ&hl=en">Alexander Lyzhov*</a>,
              <a href="">Yuliya Molchanova*</a>,
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>UAI</em>, 2020  
              <br>
                <a href="https://github.com/bayesgroup/gps-augment">code</a> / 
                <a href="https://arxiv.org/abs/2002.09103">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/gps_uai20/paper.txt" target="_blank">bibtex</a>
              <p></p>
              <p>
            We introduce greedy policy search (GPS), a simple but high-performing method for learning a policy of test-time augmentation.   
            </td>
          </tr> 

    
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/pitfalls_unc_ens_iclr20/pic.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=BJxI5gHKDr">
                <papertitle>Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning</papertitle>
              </a>
              <br>
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.ru/citations?user=5LXTi40AAAAJ&hl=en">Alexander Lyzhov*</a>,
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov*</a>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>ICLR</em>, 2020  
              <br>
              <a href="https://senya-ashukha.github.io/pitfalls-uncertainty&ensembling">blog post</a> / 
                <a href="https://iclr.cc/virtual_2020/poster_BJxI5gHKDr.html">poster video (5mins)</a> / 
                <a href="https://github.com/bayesgroup/pytorch-ensembles">code</a> / 
                <a href="https://arxiv.org/abs/2002.06470">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/pitfalls_unc_ens_iclr20/paper.txt" target="_blank">bibtex</a>
              <p></p>
              <p>
            The work introduces <i>calibrated log-likelihood</i> a reliable uncertainty estimation metric, <i>deep ensemble equivalent</i> an interpretable technique for comparison of ensembles, and points out that <i>test-time augmentation</i>  is a simple technique that allows to improve ensembles for free.</p>  
            </td>
          </tr> 

          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/dwp_iclr19/dwp.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=ByGuynAct7">
                <papertitle>The Deep Weight Prior</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=XriU_R8AAAAJ&hl=en">Andrei Atanov*</a>,
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.com/citations?user=q69zIO0AAAAJ&hl=en">Kirill Struminsky</a>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>,
              <a href="https://staff.fnwi.uva.nl/m.welling/">Max Welling</a>
              <br>
                <em>ICLR</em>, 2019  
              <br>
                <a href="https://github.com/bayesgroup/deep-weight-prior">code</a> /
                <a href="https://arxiv.org/abs/1810.06943">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/dwp_iclr19/paper.txt" target="_blank">bibtex</a> 
              <p></p>
              <p>
              The <i>deep weight prior</i> is the generative model for kernels of convolutional neural networks, that acts as a prior distribution while training on new datasets.</p>
            </td>
          </tr> 

          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/vn-iclr19/vn.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=B1GAUs0cKQ">
                <papertitle>Variance Networks: When Expectation Does Not Meet Your Expectations</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.ru/citations?user=eOttYWgAAAAJ&hl=en">Kirill Neklyudov*</a>,
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov*</a>,
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>ICLR</em>, 2019  
              <br>
                <a href="https://github.com/da-molchanov/variance-networks">code</a> /
                <a href="https://arxiv.org/abs/1803.03764">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/vn-iclr19/paper.txt" target="_blank">bibtex</a> 
              <p></p>
              <p>
              It is possible to learn a zero-centered Gaussian distribution over the weights of a neural network by learning only variances, and it works surprisingly well.</p>
            </td>
          </tr> 

            <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/sbp_neurips17/sbp.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="http://papers.nips.cc/paper/7254-structured-bayesian-pruning-via-log-normal-multiplicative-noise">
                <papertitle>Structured Bayesian Pruning via Log-Normal Multiplicative Noise</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.ru/citations?user=eOttYWgAAAAJ&hl=en">Kirill Neklyudov</a>,
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov</a>,
              <strong>Arsenii Ashukha</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>NeurIPS</em>, 2017  
              <br>
               <a href="https://github.com/necludov/group-sparsity-sbp">code</a> / 
                <a href="https://arxiv.org/abs/1705.07283">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/sbp_neurips17/paper.txt" target="_blank">bibtex</a> / 
                <a href="https://bayesgroup.github.io/pdf/sbp-poster.pdf">poster</a> 
              <p></p>
              <p>
              The model allows to sparsify a DNN with an arbitrary pattern of spasticity e.g., neurons or convolutional filters. 
              
            </td>
          </tr> 

          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/svdo_icml17/svdo_prev.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="http://proceedings.mlr.press/v70/molchanov17a.html">
                <papertitle>Variational Dropout Sparsifies Deep Neural Networks</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov*</a>,
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>ICML</em>, 2017  
              <br>
                <a href="https://iclr.cc/virtual_2020/poster_BJxI5gHKDr.html">talk (15 mins)</a> /  
                <a href="https://arxiv.org/abs/1701.05369">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/svdo_icml17/paper.txt" target="_blank">bibtex</a> / 
                code (<a href="https://github.com/bayesgroup/variational-dropout-sparsifies-dnn">Theano</a>,
                <a href="https://github.com/google-research/google-research/tree/master/state_of_sparsity/layers/variational_dropout" target="_blank">TF by GoogleAI</a>,
                <a href="https://colab.research.google.com/github/bayesgroup/deepbayes-2019/blob/master/seminars/day6/SparseVD-solution.ipynb" target="_blank">Colab PyTorch</a>) /
                 <a href="javascript:toggle_vis('contact2')"  style="color:gray"><b>retrospective</b></a>
                  <div id="contact2" style="display: none;"> 
                    <br>
                    <br>
                    <b>Retrospective</b>: 
                    <i>i)</i> SparsesVD indeed works. I have about quite a few practical use cases. However, careful usage of pruning often <a href="https://arxiv.org/abs/1902.09574 ">can produce</a> better results. 
                    <i>ii)</i> Training of deep models with noise is known to be hard and unstable. That is less the case with SparseVD. All variances, by a happy coincidence, have been initialized with small values and did not change much during training. Using small constant variances does not hurt the performance, so SparseVD might be considered as a fancy regulariser with no noise. 
                    <i>iii)</i> The sparse solution is just a local optimum, as better values of ELBO <a href="https://openreview.net/forum?id=B1GAUs0cKQ">can be achieved</a> with a <ins>less</ins> flexible variational posterior q(w_ij)=N(w_ij | 0, Ïƒ_ij).
                   </p>
                  </div>
              <p></p>
              <p>
              Variational dropout secretly trains highly sparsified deep neural networks, while a pattern of sparsity is learned jointly with weights during training.</p>
            </td>
          </tr> 
          </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p align="right">
                The webpage template was borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">Jon Barron</a>.
                <br>
                Also, check out his research, it is very interesting!
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
