<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Senya is Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-167092781-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-167092781-1');
  </script>

  <script type="text/javascript">
   function visibility_on(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'none')
            e.style.display = 'block';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'none')
            e.style.display = 'block';
   }
   function visibility_off(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'block')
            e.style.display = 'none';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'block')
            e.style.display = 'none';
   }
   function toggle_visibility(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
   }
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
</script>


  <title>Senya Ashukha</title>
  
  <meta name="author" content="Senya Ashukha">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Arsenii Ashukha</name>
              </p>
              <p>
                <br>
                I am a Research Scientist at Samsung AI Center. I (almost) received a PhD in  Machine Learning, so I can make big overcomplicated DNNs work üôÇ. The results of my PhD contributed to sparsification, uncertainty estimation, ensembling, and fundamentals of Bayesian deep learning. 
                <br>
                <br>
                Prior to that, I was a part of Yandex Research in collaboration with University of Amsterdam, where I worked on Bayesian deep learning with Dmitry Vetrov and Max Welling. I did ML engineering internships at Yandex (deep learning for music), Rambler (recommendation systems), and worked on NLP with Natalia Loukachevitch.
              </p>
              <p align=center>
                <a href="mailto:ars.ashuha@gmail.com" target="_blank">Email</a> &nbsp/&nbsp
                <a href="https://senya-ashukha.github.io/arsenii-ashukha-cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=IU-kuP8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/senya-ashukha">GitHub</a> &nbsp/&nbsp
                <a href="https://twitter.com/senya_ashuha">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/senya-ashukha.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/senya-ashukha.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Research</heading>
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/lama_21/ezgif-7-d6037b7aa186.gif' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle" bgcolor="#ffffff">
              <a href="https://arxiv.org/abs/2109.07161">
                <papertitle>Resolution-robust Large Mask Inpainting with Fourier Convolutions</papertitle>
              </a>
              <br>
             <a href="https://scholar.google.com/citations?user=HvVrLNwAAAAJ&hl=en">Roman Suvorov</a>, <a href="https://scholar.google.com/citations?hl=en&user=pf5LuKkAAAAJ">Elizaveta Logacheva</a>, <a href="https://www.kaggle.com/heyt0ny">Anton Mashikhin</a>, <a href="https://ru.linkedin.com/in/aremizova"> Anastasia Remizova</a>, 
             <strong>Arsenii Ashukha</strong>, <a href="https://dblp.org/pid/261/3035.html">Aleksei Silvestrov</a>, <a href="https://kr.linkedin.com/in/naejin-kong-654a58ba">Naejin Kong</a>, <a href="https://in.linkedin.com/in/harshithgoka">Harshith Goka</a>, <a href="https://kr.linkedin.com/in/kiwoong-park-67b02218">Kiwoong Park</a>, <a href="https://scholar.google.com/citations?user=gYYVokYAAAAJ&hl=en">Victor Lempitsky</a>
              <br>
                <em>WACV</em>, 2022  
              <br>
                <a href="https://saic-mdal.github.io/lama-project/">project page</a> / 
                <a href="https://arxiv.org/abs/2109.07161">arXiv</a> / 
                <a href="https://github.com/saic-mdal/lama">code</a> / 
                <a href="https://senya-ashukha.github.io/projects/lama_21/paper.txt" target="_blank">bibtex</a>
              <p>
              Our model generalizes surprisingly well to much higher resolutions (~2k‚ùóÔ∏è) than it saw during training (256x256), and achieves the excellent performance even in challenging scenarios, e.g. completion of periodic structures.</p>
            </td>
          </tr>
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/pitfalls_unc_ens_iclr20/pic.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle" bgcolor="#ffffff">
              <a href="https://openreview.net/forum?id=BJxI5gHKDr">
                <papertitle>Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning </papertitle>
              </a>
              <br>
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.ru/citations?user=5LXTi40AAAAJ&hl=en">Alexander Lyzhov*</a>,
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov*</a>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>ICLR</em>, 2020  
              <br>
              <a href="https://senya-ashukha.github.io/pitfalls-uncertainty&ensembling">blog post</a> / 
                <a href="https://iclr.cc/virtual_2020/poster_BJxI5gHKDr.html">poster video (5mins)</a> / 
                <a href="https://github.com/bayesgroup/pytorch-ensembles">code</a> / 
                <a href="https://arxiv.org/abs/2002.06470">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/pitfalls_unc_ens_iclr20/paper.txt" target="_blank">bibtex</a>
              <p></p>
              <p>
            The work shows that i) a simple ensemble of independently trained networks performs significantly better than recent techniques ii) a simple test-time augmentation applied to a conventional network outperforms low-parameters ensembles (e.g. Dropout) and also improves all ensembles for free iii) comparison of uncertainty estimation ability of algorithms is often done incorectly in literature. </p>  
            </td>
          </tr> 
<tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:10px;width:25%;vertical-align:middle">
                <img src='projects/gps_uai20/gps.png' width="170px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="http://proceedings.mlr.press/v124/lyzhov20a.html">
                <papertitle>Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov*</a>,
              <a href="https://scholar.google.ru/citations?user=5LXTi40AAAAJ&hl=en">Alexander Lyzhov*</a>,
              <a href="">Yuliya Molchanova*</a>,
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>UAI</em>, 2020  
              <br>
                <a href="https://github.com/bayesgroup/gps-augment">code</a> / 
                <a href="https://arxiv.org/abs/2002.09103">arXiv</a> / 
               <a href="https://senya-ashukha.github.io/projects/gps_uai20/gps_uai20_slides.pdf" target="_blank">slides</a> /
                <a href="https://senya-ashukha.github.io/projects/gps_uai20/paper.txt" target="_blank">bibtex</a>
              <p></p>
              <p>
            We introduce greedy policy search (GPS), a simple but high-performing method for learning a policy of test-time augmentation.   
            </td>
          </tr>
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/dwp_iclr19/dwp.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=ByGuynAct7">
                <papertitle>The Deep Weight Prior</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=XriU_R8AAAAJ&hl=en">Andrei Atanov*</a>,
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.com/citations?user=q69zIO0AAAAJ&hl=en">Kirill Struminsky</a>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>,
              <a href="https://staff.fnwi.uva.nl/m.welling/">Max Welling</a>
              <br>
                <em>ICLR</em>, 2019  
              <br>
                <a href="https://github.com/bayesgroup/deep-weight-prior">code</a> /
                <a href="https://arxiv.org/abs/1810.06943">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/dwp_iclr19/paper.txt" target="_blank">bibtex</a> 
              <p></p>
              <p>
              The <i>deep weight prior</i> is the generative model for kernels of convolutional neural networks, that acts as a prior distribution while training on new datasets.</p>
            </td>
          </tr> 

          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/vn-iclr19/vn.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=B1GAUs0cKQ">
                <papertitle>Variance Networks: When Expectation Does Not Meet Your Expectations</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.ru/citations?user=eOttYWgAAAAJ&hl=en">Kirill Neklyudov*</a>,
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov*</a>,
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>ICLR</em>, 2019  
              <br>
                <a href="https://github.com/da-molchanov/variance-networks">code</a> /
                <a href="https://arxiv.org/abs/1803.03764">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/vn-iclr19/paper.txt" target="_blank">bibtex</a> 
              <p></p>
              <p>
              It is possible to learn a zero-centered Gaussian distribution over the weights of a neural network by learning only variances, and it works surprisingly well.</p>
            </td>
          </tr> 
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/nfssl_innf19/nfssl.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="https://invertibleworkshop.github.io/INNF_2019/accepted_papers/pdfs/INNF_2019_paper_20.pdf">
                <papertitle>Semi-Conditional Normalizing Flows for Semi-Supervised Learning</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=XriU_R8AAAAJ&hl=en">Andrei Atanov</a>,              
              <a href="https://alexandravolokhova.github.io/">Alexandra Volokhova</a>,
              <strong>Arsenii Ashukha</strong>,
              <a href="https://isosnovik.xyz/">Ivan Sosnovik</a>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>INNF Workshop at ICML</em>, 2019  
              <br>
                <a href="https://github.com/AndrewAtanov/semi-supervised-flow-pytorch">code</a> /
                <a href="https://arxiv.org/abs/1905.00505">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/nfssl_innf19/paper.txt" target="_blank">bibtex</a> 
              <p></p>
              <p>We employ semi-conditional normalizing flow architecture that allows efficiently trains normalizing flows when only few labeled data points are available.</p>
            </td>
          </tr> 

            <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/dyn_nips19/dyn_aae.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="http://bayesiandeeplearning.org/2019/papers/102.pdf">
                <papertitle>Unsupervised Domain Adaptation with SharedLatent Dynamics for Reinforcement Learning</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=XriU_R8AAAAJ&hl=en">Evgenii Nikishin</a>,              
              <strong>Arsenii Ashukha</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>BLD Workshop at NeurIPS</em>, 2019  
              <br>
                <a href="https://github.com/evgenii-nikishin/dyn_aae">code</a> /
                <a href="https://evgenii-nikishin.github.io/data/dyn_aae_poster.pdf">poster</a> 
              <p></p>
              <p>Domain adaptation via learning shared dynamics in a latent space with adversarial matching of latent states.</p>
            </td>
          </tr> 
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/sbn_iclrw18/sbn.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=r1yXEdkvz">
                <papertitle>Uncertainty Estimation via Stochastic Batch Normalization</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=XriU_R8AAAAJ&hl=en">Andrei Atanov</a>,
              <strong>Arsenii Ashukha</strong>,
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov</a>,
              <a href="https://scholar.google.ru/citations?user=eOttYWgAAAAJ&hl=en">Kirill Neklyudov</a>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>Joint Workshop Track at ICLR</em>, 2018  
              <br>
                <a href="https://github.com/AndrewAtanov/stochastic-batch-normalization">code</a> /
                <a href="https://arxiv.org/abs/1802.04893">arXiv</a>
              <p></p>
              <p>Inference-time stochastic batch normalization improves the performance of uncertainty estimation of ensembles.</p>
            </td>
          </tr> 

            <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/sbp_neurips17/sbp.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle">
              <a href="http://papers.nips.cc/paper/7254-structured-bayesian-pruning-via-log-normal-multiplicative-noise">
                <papertitle>Structured Bayesian Pruning via Log-Normal Multiplicative Noise</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.ru/citations?user=eOttYWgAAAAJ&hl=en">Kirill Neklyudov</a>,
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov</a>,
              <strong>Arsenii Ashukha</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>NeurIPS</em>, 2017  
              <br>
               <a href="https://github.com/necludov/group-sparsity-sbp">code</a> / 
                <a href="https://arxiv.org/abs/1705.07283">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/sbp_neurips17/paper.txt" target="_blank">bibtex</a> / 
                <a href="https://bayesgroup.github.io/pdf/sbp-poster.pdf">poster</a> 
              <p></p>
              <p>
              The model allows to sparsify a DNN with an arbitrary pattern of spasticity e.g., neurons or convolutional filters. 
              
            </td>
          </tr> 

          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffff">
            <td style="padding:15px;width:25%;vertical-align:middle">
                <img src='projects/svdo_icml17/svdo_prev.png' width="160px">
            </td>
            <td style="padding:15px;width:75%;vertical-align:middle" bgcolor="#ffffff">
              <a href="http://proceedings.mlr.press/v70/molchanov17a.html">
                <papertitle>Variational Dropout Sparsifies Deep Neural Networks</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=tJ6JXRYAAAAJ&hl=en">Dmitry Molchanov*</a>,
              <strong>Arsenii Ashukha*</strong>,
              <a href="https://scholar.google.com/citations?user=7HU0UoUAAAAJ&hl=en">Dmitry Vetrov</a>
              <br>
                <em>ICML</em>, 2017  
              <br>
               <a href="javascript:toggle_vis('contact2')"  style="color:gray"><b>retrospective‚è≥</b></a> /
                <a href="https://iclr.cc/virtual_2020/poster_BJxI5gHKDr.html">talk (15 mins)</a> /  
                <a href="https://arxiv.org/abs/1701.05369">arXiv</a> / 
                <a href="https://senya-ashukha.github.io/projects/svdo_icml17/paper.txt" target="_blank">bibtex</a> / 
                code (<a href="https://github.com/bayesgroup/variational-dropout-sparsifies-dnn">theano</a>,
                <a href="https://github.com/google-research/google-research/tree/master/state_of_sparsity/layers/variational_dropout" target="_blank">tf by GoogleAI</a>,
                <a href="https://colab.research.google.com/github/bayesgroup/deepbayes-2019/blob/master/seminars/day6/SparseVD-solution.ipynb" target="_blank">colab pytorch</a>)
                  <div id="contact2" style="display: none;"> 
                    <br>
                    <br>
                    <b>Retrospective</b>: 
                    <ul>
                    <li></i> SparsesVD works in practice and it was used for network sparsification in leading IT companies. However, future studies showed that careful usage of pruning-based methods <a href="https://arxiv.org/abs/1902.09574">can produce</a> better results. </li>
                    <li></i> Training of deep models with noise is known to be hard and unstable. That is less the case with SparseVD. All variances are initialized with small values and did not change much during training. Using small variances does not hurt the performance, thus SparseVD might be considered as a fancy regulariser with (almost) no noise. </li>
                    <li></i> The sparse solution is just a local optimum, as better values of ELBO <a href="https://openreview.net/forum?id=B1GAUs0cKQ">can be achieved</a> with a <ins>less</ins> flexible variational posterior q(w_ij)=N(w_ij | 0, œÉ_ij).</li>
                   </ul>
                      </p>
                  </div>
              <p></p>
              <p>
              Variational dropout secretly trains highly sparsified deep neural networks, while a pattern of sparsity is learned jointly with weights during training.</p>
            </td>
          </tr> 
 
          </table>
        </div>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <heading>Code</heading>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p align="left">
              Check out very short and simple and fan to make implementations of ML algorithms:
                <ul>
                <li><a href="https://github.com/senya-ashukha/simple-gradient-boosting">Gradient Boosting</a></li>
                <li><a href="https://github.com/senya-ashukha/real-nvp-pytorch">Real NVP</a></li>
                <li><a href="https://github.com/senya-ashukha/quantile-regression-dqn-pytorch">Quantile Regression DQN (Distributional RL)</a></li>
              </ul>
              <br>
              Also, chek out more solid implementations (at least they can do ImageNet):
              <ul>
                <li><a href="https://github.com/AndrewAtanov/simclr-pytorch">Multi-gpu SimCLRv1</a></li>
                <li><a href="https://github.com/bayesgroup/pytorch-ensembles">Ensembles (Deep ensembles, Snapshot ensembles, cSGLD, FGE, etc.)</li>
              <ul>
              </p>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p align="right" class="container">
                The webpage template was borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">Jon Barron</a>.
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
